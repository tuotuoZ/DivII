---
title: "Multicultural Perspective"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Multiple Cultural Perspective

In one of the community education day, I was lucky to attend one of the talk about software discrimination from Professor Yuriy Brun. He asked a question to us: Can software discriminate? It sounded really interesting to me. Machines don’t have motion and don’t even have a preference, so how could they discriminate? Professor Brun give us some some examples. The first one was an application to decide who can get the loan and who cannot and it was discriminating based the location of the applicants. The second one was a risk assessment software to determine if a criminal should go to jail or set free. The data showed that the algorithm was biased and against African American. Testing if one software can be trivial. In order to test that, Professor Brun showed some techniques. He said one of the techniques was to flip some of the important identifier such as, race, gender and zip code. They used the original data but flipped the race from Black to White and White to Black. What had been found out was that changing this one factor in data gave a drastically different result, which could show that the algorithm was biased. The last example was about Amazon’s delivery system discriminated people’s race based on their zip code. The lesson from Amazon’s example was that no one even input users’ race in the data. Somehow, the algorithm figured out that the people live in this area had the same race. So, even if you factor out the race data, it’s still not even to give a non-discriminating software and you have to intentionally tuned that.

After the talk, I was surprised how the algorithm can find out the race based on the zip code, so I put this as my mid-term project for my Intro to Data Science class. I digged more paper and I realized that the algorithm analyzed the crime rate and income level for a area. It is motivating to see that social injustice was addressed so we can have a chance to delivery justice. Solving this problem does not only require computer science knowledge, but also require history, economics, and data science. It’s a great example of interdisciplinary study which I could have a chance to combine some of my knowledge into a question in real life. It is also a great lesson for multiple cultural perpectives. I should always remind myself paying attention to the cultural and racial problem even if I didn’t put some factors like race and gender into my algorithm model. I should actively review my system to see if there is a potential bias against a certain race.
